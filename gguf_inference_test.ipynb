{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference\n",
    "\n",
    "langchain을 통한 inference\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/llamacpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain - bllossom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "bllossom = LlamaCpp(\n",
    "    model_path=\"bllossom/llama-3-Korean-Bllossom-8B-Q5_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    temperature=0.6,\n",
    "    top_p=1,\n",
    "    max_tokens=2048,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "mistral = LlamaCpp(\n",
    "    model_path=\"./mistral/ggml-model-q5_k_m.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    temperature=0.6,\n",
    "    top_p=1,\n",
    "    max_tokens=128,\n",
    "    stop=[\"Person1:\", \"Person2:\"],\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"주어진 Persona를 가진 사람으로 1개의 문장으로 된 답변을 생성해주세요.\\nPersona:{context_list}\\ndialog:{src_list}\\nPerson2:\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = '\\nPerson1:안녕하십니까! 저는 20대 여자입니다~\\nPerson2:네~즐거운 대화해요~!! 저는 40대 여자입니다~\\nPerson1:무슨일 하시나요? 저는 중학교 수학교인데, 요즘 바쁜 시기에요.'\n",
    "context_list = '나는 여자이다.\\n나는 40대이다.\\n나는 가족과 함께하는 시간이 많다.\\n나는 비글 두 마리를 키운다.\\n나는 요즘, 자주 상처를 받는다.\\n나는 40대 여자이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_chain = LLMChain(prompt=prompt, llm=solar)\n",
    "prediction = solar_chain.run({'src_list' : src_list, 'context_list' : context_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "prediction = mistral_chain.run({'src_list' : src_list, 'context_list' : context_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"주어진 페르소나를 가진 사람이 되어 이전 대화 맥락에 맞게 답변해주세요. \\n페르소나:{context_list}\\n이전 대화:{src_list}\\nPerson2:\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = '\\nPerson1:안녕하십니까! 저는 20대 여자입니다~\\nPerson2:네~즐거운 대화해요~!! 저는 40대 여자입니다~\\nPerson1:무슨일 하시나요? 저는 중학교 수학교인데, 요즘 바쁜 시기에요.'\n",
    "context_list = '나는 여자이다.\\n나는 40대이다.\\n나는 가족과 함께하는 시간이 많다.\\n나는 비글 두 마리를 키운다.\\n나는 요즘, 자주 상처를 받는다.\\n나는 40대 여자이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_chain = LLMChain(prompt=prompt, llm=solar)\n",
    "solar_chain.run({'src_list' : src_list, 'context_list' : context_list})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "true_friend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
